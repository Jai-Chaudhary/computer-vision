{
 "metadata": {
  "name": "",
  "signature": "sha256:f22297580d7936bfad662d608318317210f7dabd3fb21c6164373a419f1648c5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "import cv2\n",
      "from skimage.measure import  ransac\n",
      "from skimage.transform import warp, AffineTransform\n",
      "from skimage import img_as_ubyte\n",
      "\n",
      "class SIFT:\n",
      "    MIN_MATCH_COUNT = 10\n",
      "    extractor = cv2.SIFT()\n",
      "\n",
      "    \n",
      "    \"\"\"\n",
      "    returns list of DMatch Objects (matches) sorted by distance \n",
      "    that contains keypoints matched in images.\n",
      "\n",
      "    :param keypoints1: list of keypoint objects for image 1\n",
      "    :param descriptorSet1:  list of descriptor objects for image 1\n",
      "    :param keypoints2:  list of keypoint objects for image 2\n",
      "    :param descriptorSet2:  list of descriptor objects for image 2\n",
      "    :returns: list of DMatch Objects\n",
      "    \"\"\"\n",
      "    def findMatches(self,keypoints1, descriptorSet1, keypoints2, descriptorSet2):        \n",
      "        # create BFMatcher object\n",
      "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
      "\n",
      "        # Match descriptors.\n",
      "        matches = bf.match(descriptorSet1,descriptorSet2)\n",
      "\n",
      "        # Sort them in the order of their distance.\n",
      "        matches = sorted(matches, key = lambda x:x.distance)\n",
      "\n",
      "        # visualize the matches\n",
      "        print '#matches:', len(matches)\n",
      "        dist = [m.distance for m in matches]\n",
      "\n",
      "        print 'distance: min: %.3f' % min(dist)\n",
      "        print 'distance: mean: %.3f' % (sum(dist) / len(dist))\n",
      "        print 'distance: max: %.3f' % max(dist)\n",
      "        \n",
      "        return matches\n",
      "\n",
      "    \"\"\"\n",
      "     generate an image where the matches are visualizated by \n",
      "     connecting matching pixels by highlighted lines\n",
      "\n",
      "    :param imageFilename1: path to image1\n",
      "    :param imageFilename2:  path to image2\n",
      "    :returns: void\n",
      "    \"\"\"\n",
      "    def showImages(self, imageFilename1, imageFilename2):\n",
      "\n",
      "\n",
      "        # find the keypoints and descriptors with SIFT\n",
      "        img1 = cv2.cvtColor(cv2.imread(imageFilename1), cv2.COLOR_BGR2GRAY)\n",
      "        img2 = cv2.cvtColor(cv2.imread(imageFilename2), cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "        kp1, des1 = self.extractor.detectAndCompute(img1,None)\n",
      "        kp2, des2 = self.extractor.detectAndCompute(img2,None)\n",
      "        matches = self.findMatches(kp1, des1, kp2, des2)\n",
      "        self.drawMatches(img1, kp1, img2, kp2, matches)\n",
      "                \n",
      "\n",
      "        \n",
      "        \n",
      "    \"\"\"\n",
      "     Visualization\n",
      "     My own implementation of cv2.drawMatches as brew binary for MacOSX\n",
      "     OpenCV 2.4.11 does not suppourt this function  but it's supported in\n",
      "     OpenCV 3.0.0\n",
      "\n",
      "    :param img1: image1 Object\n",
      "    :param kp1: list of keypoint objects for image 1\n",
      "    :param img2: image2 Object\n",
      "    :param kp2: list of keypoint objects for image 2\n",
      "    :param matches: list of DMatch Objects that contains keypoints matched in images.\n",
      "    :returns: void\n",
      "    \"\"\"    \n",
      "    def drawMatches(self, img1, kp1, img2, kp2, matches, name = 'DrawMatches'):\n",
      "\n",
      "        \n",
      "        h1, w1 = img1.shape[:2]\n",
      "        h2, w2 = img2.shape[:2]\n",
      "        view = sp.zeros((max(h1, h2), w1 + w2, 3), sp.uint8)\n",
      "        view[:h1, :w1, 0] = img1\n",
      "        view[:h2, w1:, 0] = img2\n",
      "        view[:, :, 1] = view[:, :, 0]\n",
      "        view[:, :, 2] = view[:, :, 0]\n",
      "\n",
      "        \n",
      "        for m in matches:\n",
      "\n",
      "            # Get the matching keypoints for each of the images\n",
      "            img1_idx = m.queryIdx\n",
      "            img2_idx = m.trainIdx\n",
      "\n",
      "            # x - columns\n",
      "            # y - rows\n",
      "            (x1,y1) = kp1[img1_idx].pt\n",
      "            (x2,y2) = kp2[img2_idx].pt\n",
      "            color = tuple([sp.random.randint(0, 255) for _ in xrange(3)])\n",
      "            cv2.line(view, (int(x1), int(y1)) , ((int(x2) + w1), int(y2)), color)\n",
      "            cv2.circle(view, (int(x1),int(y1)), 4, (255, 0, 0), 1)   \n",
      "            cv2.circle(view, (int(x2)+w1,int(y2)), 4, (255, 0, 0), 1)\n",
      "\n",
      "        cv2.imwrite(name + \".png\", view)\n",
      "\n",
      "        \n",
      "    def affineMatches(self, matches, kp1, kp2):\n",
      "        src_pts = np.float32([ kp1[m.queryIdx].pt for m in matches ])\n",
      "        dst_pts = np.float32([ kp2[m.trainIdx].pt for m in matches ])\n",
      "        model = AffineTransform()\n",
      "        model.estimate(src_pts, dst_pts)\n",
      "        \n",
      "        model_robust, inliers = ransac((src_pts, dst_pts), AffineTransform, min_samples=3,\n",
      "                               residual_threshold=30, max_trials=500)\n",
      "        \n",
      "        print(\"Least Square Model(scale, translation. rotation) \", model.scale, model.translation, model.rotation)\n",
      "        print(\"RANSAC Least Square Model(scale, translation. rotation) \", model_robust.scale, model_robust.translation, model_robust.rotation)\n",
      "\n",
      "        inlier_idxs = np.nonzero(inliers)[0]\n",
      "        \n",
      "        affineMatches = [matches[idx] for idx in inlier_idxs]\n",
      "        print (\"AFFINE MATCHES: \", len(affineMatches))\n",
      "\n",
      "        return affineMatches, model_robust\n",
      "\n",
      "    def alignAffine(self, imageFilename1, imageFilename2, affineTransform):\n",
      "        img1 = cv2.imread(imageFilename1)\n",
      "        img2 = cv2.imread(imageFilename2)\n",
      "        img_warped = warp(img2, affineTransform, output_shape=img1.shape)\n",
      "\n",
      "        b1,g1,r1 = cv2.split(img1)\n",
      "        \n",
      "        b_warped,g_warped,r_warped = cv2.split(img_warped)\n",
      "        img_merged = cv2.merge((img_as_ubyte(b_warped), g1, r1 ))\n",
      "        cv2.imshow(\"AffineorigImage.png\", img1)\n",
      "        cv2.imshow(\"AffinewarpedImage.png\", img_as_ubyte(img_warped))\n",
      "        cv2.imshow(\"AffinemergedImage.png\", img_merged)\n",
      "\n",
      "        print \"Affine Error: \", self.alignmentError(img1, img_warped)\n",
      "        \n",
      "    def alignmentError(self, img1, img2):\n",
      "        hist1 = cv2.calcHist([img1.astype('float32')],[0],None,[256],[0,256])\n",
      "        hist2 = cv2.calcHist([img2.astype('float32')],[0],None,[256],[0,256])\n",
      "        return cv2.compareHist(hist1, hist2, cv2.cv.CV_COMP_BHATTACHARYYA)\n",
      "        \n",
      "    def homographyMatches(self, matches, kp1, kp2):\n",
      "        src_pts = np.float32([ kp1[m.queryIdx].pt for m in matches ]).reshape(-1,1,2)\n",
      "        dst_pts = np.float32([ kp2[m.trainIdx].pt for m in matches ]).reshape(-1,1,2)\n",
      "\n",
      "        transformMatrix, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,30.0)\n",
      "        \n",
      "        inliers = mask.ravel().tolist()\n",
      "        \n",
      "        inlier_idxs = np.nonzero(inliers)[0]\n",
      "        \n",
      "        homographyMatches = [matches[idx] for idx in inlier_idxs]\n",
      "        \n",
      "        print (\"Homography Matches\", len(homographyMatches))\n",
      "        return homographyMatches, transformMatrix\n",
      "    \n",
      "    \n",
      "    def alignHomography(self, imageFilename1, imageFilename2, homographyTransform):\n",
      "        img1 = cv2.imread(imageFilename1)\n",
      "        img2 = cv2.imread(imageFilename2)\n",
      "        img_warped = cv2.warpPerspective(img2, homographyTransform, img1.shape[:2], (1000,1000), flags = cv2.WARP_INVERSE_MAP)\n",
      "\n",
      "        b1,g1,r1 = cv2.split(img1)\n",
      "        \n",
      "        b_warped,g_warped,r_warped = cv2.split(img_warped)\n",
      "\n",
      "        img_merged = cv2.merge((b_warped,g1,r1))\n",
      "        cv2.imwrite(\"HomographyorigImage.png\", img1)\n",
      "        cv2.imwrite(\"HomographywarpedImage.png\", img_warped)\n",
      "        cv2.imwrite(\"HomographymergedImage.png\", img_merged)\n",
      "\n",
      "        print \"Homography Error: \", self.alignmentError(img1, img_warped)\n",
      "        \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    sift = SIFT()\n",
      "    imageFilename1 = 'StopSign1.jpg'\n",
      "    imageFilename2 = 'StopSign4.jpg'\n",
      "\n",
      "    sift.showImages(imageFilename1, imageFilename2)\n",
      "\n",
      "    \n",
      "    img1 = cv2.cvtColor(cv2.imread(imageFilename1), cv2.COLOR_BGR2GRAY)\n",
      "    img2 = cv2.cvtColor(cv2.imread(imageFilename2), cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "    kp1, des1 = sift.extractor.detectAndCompute(img1,None)\n",
      "    kp2, des2 = sift.extractor.detectAndCompute(img2,None)\n",
      "    matches = sift.findMatches(kp1, des1, kp2, des2)\n",
      "    affineMatches, affineTransform =  sift.affineMatches(matches, kp1, kp2)\n",
      "\n",
      "    sift.drawMatches(img1, kp1, img2, kp2, affineMatches, 'affineMatches')\n",
      "\n",
      "    sift.alignAffine( imageFilename1, imageFilename2, affineTransform)\n",
      "\n",
      "    homographyMatches,  homographyTransform = sift.homographyMatches(matches, kp1, kp2)\n",
      "\n",
      "    sift.drawMatches(img1, kp1, img2, kp2, homographyMatches, 'homographyMatches')\n",
      "\n",
      "    sift.alignHomography( imageFilename1, imageFilename2, homographyTransform)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#matches: 57\n",
        "distance: min: 66.182\n",
        "distance: mean: 248.593\n",
        "distance: max: 419.063\n",
        "#matches:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 57\n",
        "distance: min: 66.182\n",
        "distance: mean: 248.593\n",
        "distance: max: 419.063\n",
        "('Least Square Model(scale, translation. rotation) ', (2.2874197495623, 51.37786614914854), array([ 1192.76605199,  7763.4657654 ]), 0.869802576575406)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('RANSAC Least Square Model(scale, translation. rotation) ', (2.593003672413597, 2.578301575009895), array([ 146.13314982,   62.798367  ]), 0.12204859657898029)\n",
        "('AFFINE MATCHES: ', 18)\n",
        "Affine Error: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.430624746516\n",
        "('Homography Matches', 18)\n",
        "Homography Error: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.881697945121\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "cv2.destroyAllWindows()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}